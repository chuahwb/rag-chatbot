# RAG Chatbot — sample environment configuration
# Copy this file to `.env` and fill in the placeholders as needed.

# -----------------------------------------------------------------------------
# Core LLM / API providers
# -----------------------------------------------------------------------------

# Primary OpenAI project key used by:
# - Planner LLM (intent/slots/decision/synthesis)
# - Embeddings for products RAG
# - Outlets Text2SQL (when TEXT2SQL_PROVIDER=openai)
OPENAI_API_KEY=sk-your-openai-key-here

# Planner + LLMs
PLANNER_LLM_PROVIDER=openai        # openai | fake | local
PLANNER_MODEL=gpt-4.1-mini
PLANNER_TEMPERATURE=0.1
PLANNER_MAX_CALLS_PER_TURN=4

# -----------------------------------------------------------------------------
# Calculator tool
# -----------------------------------------------------------------------------

# Use in-process calculator by default for local/dev (no extra service needed).
CALC_TOOL_MODE=local               # local | http

# When CALC_TOOL_MODE=http, point to your external calculator service:
# CALC_HTTP_BASE_URL=http://localhost:9000
# CALC_HTTP_TIMEOUT_SEC=5

# -----------------------------------------------------------------------------
# Products RAG (drinkware FAISS index)
# -----------------------------------------------------------------------------

# Embedding provider for product documents.
EMBEDDINGS_PROVIDER=openai         # openai | fake

# Path to FAISS index directory (inside Docker container).
VECTOR_STORE_PATH=/app/data/faiss/products

# Optional product summary generation (AI-written summaries).
PRODUCT_SUMMARY_PROVIDER=openai    # none | fake | openai
PRODUCT_SUMMARY_MODEL=gpt-4.1-mini
PRODUCT_SUMMARY_TIMEOUT_SEC=8

# -----------------------------------------------------------------------------
# Outlets Text2SQL (SQLite + LLM-generated SQL)
# -----------------------------------------------------------------------------

# Provider for NL→SQL:
# - openai: use OpenAI ChatCompletion
# - local: use Ollama (OLLAMA_HOST) if running
# - fake: deterministic heuristic generator (no external calls)
TEXT2SQL_PROVIDER=openai           # openai | local | fake
TEXT2SQL_MODEL=gpt-4.1-mini
TEXT2SQL_TIMEOUT_SEC=8

# SQLite DB URL for outlets table.
# In Docker this points to the mounted volume under /app/data.
SQLITE_URL=sqlite:////app/data/sqlite/outlets.db

# Optional local Ollama endpoint when TEXT2SQL_PROVIDER=local
OLLAMA_HOST=http://localhost:11434

# -----------------------------------------------------------------------------
# Service behavior / CORS / SSE
# -----------------------------------------------------------------------------

# Enable Server-Sent Events for planner telemetry (/events).
ENABLE_SSE=true

# Allowed frontend origins (React dev server, etc.).
# Either a JSON array as below or a comma-separated string is fine.
CORS_ORIGINS=["http://localhost:5173"]

# -----------------------------------------------------------------------------
# Langfuse observability (optional)
# -----------------------------------------------------------------------------
# When set, Langfuse will receive traces for planner and LLM calls.

LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key
LANGFUSE_HOST=https://cloud.langfuse.com
LANGFUSE_RELEASE=local             # e.g. git SHA, version tag, or "local"